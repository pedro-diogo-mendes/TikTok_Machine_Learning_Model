{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9140249,"sourceType":"datasetVersion","datasetId":5520258}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **TikTok Claims Classification Machine Learning Model Project**\n![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/jfqeLHzXSmiWDsf44aBzcg_49d87872c4e54600b22020ca45545df1_image.png?expiry=1724198400000&hmac=FrrujYjU4bnsRleCWUHnmsbBw5M2E5XTUaFwu1s5ZGs)","metadata":{}},{"cell_type":"markdown","source":"Welcome to the TikTok Claims Classification Model Project!\n\nThis project is part of the Google Advanced Data Analytics Certificate Program in which we have 3 different projects that we can work with in order to finish the course.\n\nI've chosen the TikTok Claims Classification Model Project, and this notebook shows my thought process. Goes through EDA (exploring and cleaning the dataset), regression analysis, and modeling a machine learning algorithm.\n\nLet's get started by stating some important thigs first and then proceed with the actual project and what it stands for.\n\n***Note:*** *The TikTok dataset was made available as part of the course and it was created in partnership with the short-form video hosting company, TikTok. The story, all names, characters, and incidents portrayed in this project are fictitious. No identification with actual persons (living or deceased) is intended or should be inferred. And, the data shared in this project has been created for pedagogical purposes.*\n\n**Background**:\n\nTikTok is the leading destination for short-form mobile video. The platform is built to help imaginations thrive. TikTok's mission is to create a place for inclusive, joyful, and authentic content–where people can safely discover, create, and connect.\n\n**Scenario**:\n\nTikTok users have the ability to report videos and comments that contain user claims. These reports identify content that needs to be reviewed by moderators. This process generates a large number of user reports that are difficult to address quickly. \n\nTikTok is working on the development of a predictive model that can determine whether a video contains a claim or offers an opinion. With a successful prediction model, TikTok can reduce the backlog of user reports and prioritize them more efficiently.\n\n**Project Goal**:\n\nThe TikTok data team is developing a machine learning model for classifying claims made in videos submitted to the platform.","metadata":{}},{"cell_type":"markdown","source":"\n# **1. Project Proposal**\n\nBefore we start coding, it’s important to make a project proposal for the data science team over at TikTok, so that they can approve all the steps we are about to make.\nThe project proposal that I'll create will have milestones for the tasks within the claim’s classification project.\n\nYou can view the project proposal over here, in my Github profile.","metadata":{}},{"cell_type":"markdown","source":"# **2. Exploratory Data Analysis (EDA)**\n\nThe purpose of this exploratory data analysis is for me to understand the impact that videos have on TikTok users. To do so, I'm going to see some descriptive statistics and analyze variables that will showcase user engagement: views, likes, and comments counts.\n\n## **2.1. Importing the Necessary Libraries and Data Frame for EDA**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-08-19T12:16:34.558755Z","iopub.execute_input":"2024-08-19T12:16:34.559122Z","iopub.status.idle":"2024-08-19T12:16:35.601688Z","shell.execute_reply.started":"2024-08-19T12:16:34.559092Z","shell.execute_reply":"2024-08-19T12:16:35.600799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the dataset into a data frame\n\ndata = pd.read_csv(\"/kaggle/input/tiktok-dataset/tiktok_dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-08-19T12:16:36.210800Z","iopub.execute_input":"2024-08-19T12:16:36.211778Z","iopub.status.idle":"2024-08-19T12:16:36.340700Z","shell.execute_reply.started":"2024-08-19T12:16:36.211745Z","shell.execute_reply":"2024-08-19T12:16:36.339819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.2. Descriptive Statistics**","metadata":{}},{"cell_type":"code","source":"# Let's display and examine the first 10 rows of the data\n\ndata.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T12:16:40.114630Z","iopub.execute_input":"2024-08-19T12:16:40.115513Z","iopub.status.idle":"2024-08-19T12:16:40.146733Z","shell.execute_reply.started":"2024-08-19T12:16:40.115475Z","shell.execute_reply":"2024-08-19T12:16:40.145788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**These are some observations that I can make seeing these 10 rows of data:**\n\n* Each row represents a different video posted by a TikTok user. This way, we know various information’s about the videos, such as whether it is flagged as a comment video or a claim video, as well as the number of views, shares, comments, likes, downloads, it's duration in seconds as well as a transcription of the text that users have talked in that specific video.\n* We also have information about the status of the video author, for example, whether it is banned, under investigation or active.\n* Furthermore, we have information about the user verified status, weather is a verified or not verified user.\n* Basically, we have a variety of metadata about the videos and users in addition to having the videos tagged as being claims or opinions.","metadata":{}},{"cell_type":"code","source":"# Summary info\n\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T12:17:24.030309Z","iopub.execute_input":"2024-08-19T12:17:24.030955Z","iopub.status.idle":"2024-08-19T12:17:24.060586Z","shell.execute_reply.started":"2024-08-19T12:17:24.030924Z","shell.execute_reply":"2024-08-19T12:17:24.059568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's explain the information we get from this output:**\n\n* When checking the different variables, I've noticed that there are 3 data types - int64, object and float64.\n* In total, we have 12 columns with metadata about the videos and 19382 rows (videos).\n* I can see that we have some lines with missing data in some columns, namely: `claim_status`, `video_transcription_text`, `video_view_count`, `video_like_count`, `video_share_count`, `video_download_count`, `video_comment_count` and, in all of them, the same number of rows of information are missing - 298 lines. This could mean that these videos have not received any type of claim status, comments or views, for example or, that these lines are in this database by mistake.","metadata":{}},{"cell_type":"code","source":"# Summary statistics\n\ndata.describe()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T12:27:21.580018Z","iopub.execute_input":"2024-08-19T12:27:21.580426Z","iopub.status.idle":"2024-08-19T12:27:21.621150Z","shell.execute_reply.started":"2024-08-19T12:27:21.580396Z","shell.execute_reply":"2024-08-19T12:27:21.620143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.size","metadata":{"execution":{"iopub.status.busy":"2024-08-19T12:27:53.977459Z","iopub.execute_input":"2024-08-19T12:27:53.977884Z","iopub.status.idle":"2024-08-19T12:27:53.984630Z","shell.execute_reply.started":"2024-08-19T12:27:53.977855Z","shell.execute_reply":"2024-08-19T12:27:53.983644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**What can we see from these descriptive statistics?**\n\n* Looking at the table, it appears that there are some outliers, particularly in the maximum values, since they are quite far from the range values (quartiles). Furthermore, they have extremely large standard deviations!\n\nSince we know from our project proposal that the ultimate objective is to use machine learning to classify videos as either claims or opinions, a good first step towards understanding the data might therefore be examining the `claim_status` variable. Let's begin by determining how many videos there are for each different claim status.","metadata":{}},{"cell_type":"code","source":"data['claim_status'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T12:31:35.924561Z","iopub.execute_input":"2024-08-19T12:31:35.925272Z","iopub.status.idle":"2024-08-19T12:31:35.935644Z","shell.execute_reply.started":"2024-08-19T12:31:35.925234Z","shell.execute_reply":"2024-08-19T12:31:35.934755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the values are very well balanced, with an almost perfect ratio between the number of videos with opinions and videos with claims, which will be optimal for our model later.\n\nNow that we understand that the distribution of the claim status variable, it’s important to extract some information about the engagement levels of the videos. In particular, let's examine the engagement trends associated with each different claim status.\n\nI'm going to start by using Boolean masking to filter the data according to claim status, then calculate the mean and median view counts for each claim status.","metadata":{}},{"cell_type":"code","source":"# What is the average view count of videos with \"claim\" status?\n\nclaims = data[data['claim_status']=='claim']\nprint('Mean visualizations in videos with Claims:', claims['video_view_count'].mean())\nprint('Median visualizations in videos with Claims:', claims['video_view_count'].median())","metadata":{"execution":{"iopub.status.busy":"2024-08-19T12:34:47.233405Z","iopub.execute_input":"2024-08-19T12:34:47.234154Z","iopub.status.idle":"2024-08-19T12:34:47.247120Z","shell.execute_reply.started":"2024-08-19T12:34:47.234122Z","shell.execute_reply":"2024-08-19T12:34:47.245989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What is the average view count of videos with \"opinion\" status?\n\nclaims = data[data['claim_status']=='opinion']\nprint('Mean visualizations in videos with Opinions:', claims['video_view_count'].mean())\nprint('Median visualizations in videos with Opinions:', claims['video_view_count'].median())","metadata":{"execution":{"iopub.status.busy":"2024-08-19T12:34:53.742325Z","iopub.execute_input":"2024-08-19T12:34:53.743007Z","iopub.status.idle":"2024-08-19T12:34:53.756379Z","shell.execute_reply.started":"2024-08-19T12:34:53.742973Z","shell.execute_reply":"2024-08-19T12:34:53.755278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**What can we notice about the mean and media within each claim category?**\n\n* The two statistics are very close in both cases (mean and median), which indicates that there aren’t outliers when we measure visualizations grouped by claim status.\n* We can see that videos with claims have many more views than videos with opinions. On average, videos with claims have more than 500 thousand views, while videos with opinions do not even have 5 thousand views, on average.\n\nNow, let's examine trends associated with the ban status of the author.","metadata":{}},{"cell_type":"code","source":"data.groupby(by=['author_ban_status', 'claim_status']).count()[['#']]","metadata":{"execution":{"iopub.status.busy":"2024-08-19T12:40:22.218077Z","iopub.execute_input":"2024-08-19T12:40:22.218517Z","iopub.status.idle":"2024-08-19T12:40:22.243495Z","shell.execute_reply.started":"2024-08-19T12:40:22.218457Z","shell.execute_reply":"2024-08-19T12:40:22.242542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**What conclusions can we draw from here?**\n\n* I’m able to see a very strong correlation with users who make claims in their videos and those who are banned from the platform or under review, meaning that those who are banned or under review, have more videos with claims.\n* This may happen because these types of videos have greater restrictions and authors have to comply with the platform's policies.\n* However, it should be noted that there is no way to know whether videos with claims result in their users being banned or that users who post videos with claims are more likely to post videos that break the terms of service.\n* Finally, while we can use this data to draw conclusions about banned/active authors, we cannot draw conclusions about banned videos. There is no way to know whether a particular video caused it to be banned and the authors may have posted videos that complied with the terms of service.\n\nLet's continue investigating engagement levels, now focusing on `author_ban_status`.","metadata":{}},{"cell_type":"code","source":"data.groupby(by=['author_ban_status']).agg({'video_view_count': ['count', 'mean', 'median'],\n                                        'video_like_count': ['count', 'mean', 'median'],\n                                        'video_share_count': ['count', 'mean', 'median']})","metadata":{"execution":{"iopub.status.busy":"2024-08-19T12:48:07.555757Z","iopub.execute_input":"2024-08-19T12:48:07.556197Z","iopub.status.idle":"2024-08-19T12:48:07.585816Z","shell.execute_reply.started":"2024-08-19T12:48:07.556166Z","shell.execute_reply":"2024-08-19T12:48:07.584774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let’s discuss the output of this code:**\n\n* As you can see, the users who are banned from the platform or under review, can have videos with double or more the engagement levels of active users like in number of views, likes and shares.\n* Furthermore, we can see that in terms of likes and shares, the average is quite different from the median, which indicates that there are outliers, that is, videos with a lot of engagement!\n\nNow, let's create three new columns to help better understand engagement rates:\n* `likes_per_view`: represents the number of likes divided by the number of views for each video\n* `comments_per_view`: represents the number of comments divided by the number of views for each video\n* `shares_per_view`: represents the number of shares divided by the number of views for each video","metadata":{}},{"cell_type":"code","source":"# Creating 3 new columns\n\ndata['likes_per_view'] = data['video_like_count'] / data['video_view_count']\n\ndata['comments_per_view'] = data['video_comment_count'] / data['video_view_count']\n\ndata['shares_per_view'] = data['video_share_count'] / data['video_view_count']","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:16:51.266822Z","iopub.execute_input":"2024-08-19T13:16:51.267830Z","iopub.status.idle":"2024-08-19T13:16:51.276301Z","shell.execute_reply.started":"2024-08-19T13:16:51.267791Z","shell.execute_reply":"2024-08-19T13:16:51.275377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile the information\n\ndata.groupby(by=['author_ban_status', 'claim_status']).agg({'likes_per_view': ['count', 'mean', 'median'],\n                                                          'comments_per_view': ['count', 'mean', 'median'],\n                                                          'shares_per_view': ['count', 'mean', 'median']})","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:17:07.269248Z","iopub.execute_input":"2024-08-19T13:17:07.269633Z","iopub.status.idle":"2024-08-19T13:17:07.303736Z","shell.execute_reply.started":"2024-08-19T13:17:07.269602Z","shell.execute_reply":"2024-08-19T13:17:07.302719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Analyzing the output of this code, we can see that videos with claims have more likes, comments and shares per view, across all the authors ban status.\n* Furthermore, we can see that these numbers are all very similar, as videos with claims are always higher in terms of likes, comments and shares, in very similar proportions, across the 3 types of author ban status!\n* Therefore, seen from this perspective, the user's status does not matter, as long as the video contains claims.","metadata":{}},{"cell_type":"markdown","source":"## **2.3. Checking Outliers**\n\nI’m going to plot boxplots to check for outliers, to have visual representation of these values in every engagement variable.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5,1))\nsns.boxplot(x = data['video_duration_sec'])","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:24:44.724627Z","iopub.execute_input":"2024-08-19T13:24:44.724995Z","iopub.status.idle":"2024-08-19T13:24:44.880450Z","shell.execute_reply.started":"2024-08-19T13:24:44.724972Z","shell.execute_reply":"2024-08-19T13:24:44.879406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,1))\nsns.boxplot(x = data['video_view_count'])","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:24:51.211692Z","iopub.execute_input":"2024-08-19T13:24:51.212320Z","iopub.status.idle":"2024-08-19T13:24:51.402616Z","shell.execute_reply.started":"2024-08-19T13:24:51.212290Z","shell.execute_reply":"2024-08-19T13:24:51.401700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,1))\nsns.boxplot(x = data['video_like_count'])","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:24:54.134610Z","iopub.execute_input":"2024-08-19T13:24:54.135213Z","iopub.status.idle":"2024-08-19T13:24:54.328883Z","shell.execute_reply.started":"2024-08-19T13:24:54.135184Z","shell.execute_reply":"2024-08-19T13:24:54.327998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,1))\nsns.boxplot(x = data['video_comment_count'])","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:24:56.851625Z","iopub.execute_input":"2024-08-19T13:24:56.851958Z","iopub.status.idle":"2024-08-19T13:24:57.042236Z","shell.execute_reply.started":"2024-08-19T13:24:56.851935Z","shell.execute_reply":"2024-08-19T13:24:57.041323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,1))\nsns.boxplot(x = data['video_share_count'])","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:25:03.374046Z","iopub.execute_input":"2024-08-19T13:25:03.374871Z","iopub.status.idle":"2024-08-19T13:25:03.566284Z","shell.execute_reply.started":"2024-08-19T13:25:03.374839Z","shell.execute_reply":"2024-08-19T13:25:03.565221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,1))\nsns.boxplot(x = data['video_download_count'])","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:25:06.590590Z","iopub.execute_input":"2024-08-19T13:25:06.591624Z","iopub.status.idle":"2024-08-19T13:25:06.801863Z","shell.execute_reply.started":"2024-08-19T13:25:06.591589Z","shell.execute_reply":"2024-08-19T13:25:06.800920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see in these graphs, there are some variables with outliers, namely `video_download_count`, `video_share_count`, `video_comment_count` and `video_like_count`, which confirms my suspicions previously raised when I analyzed the mean and median in 2.2. Descriptive Statistics. I can also confirm that there are videos with engagement levels much higher than the average.\nNow that I’ve confirmed that we have outliers in several variables, I will later decide what to do with them when we choose the machine learning model I’ll use, as there are models that are sensitive to outliers and others that are not.\n\nWhen building predictive models, the presence of outliers can be problematic. For example, if we were trying to predict the view count of a particular video, videos with extremely high view counts might introduce bias to a model. Also, some outliers might indicate problems with how data was captured or recorded.\n\nThe ultimate objective of the TikTok project is to build a model that predicts whether a video is a claim or opinion. The analysis we've performed so far indicates that a video's engagement level is strongly correlated with its claim status. There's no reason to believe that any of the values in the TikTok data are erroneously captured, and they align with expectation of how social media works: a very small proportion of videos get super high engagement levels. That's the nature of viral content.\n\nNonetheless, it's good practice to get a sense of just how many of our data points could be considered outliers. The definition of an outlier can change based on the details of any project, and it helps to have domain expertise to decide a threshold. I've learned that a common way to determine outliers in a normal distribution is to calculate the interquartile range (IQR) and set a threshold that is 1.5 * IQR above the 3rd quartile.\n\nIn this TikTok dataset, the values for the count variables are not normally distributed, as you will see later. They are heavily skewed to the right. One way of modifying the outlier threshold is by calculating the median value for each variable and then adding 1.5 * IQR. This results in a threshold that is, in this case, much lower than it would be if we used the 3rd quartile.\n\nLet's write a for loop that iterates over the column names of each count variable and then check the distribution of these same variables, creating some histograms.","metadata":{}},{"cell_type":"code","source":"count_cols = ['video_view_count',\n              'video_like_count',\n              'video_share_count',\n              'video_download_count',\n              'video_comment_count',\n              ]\n\nfor column in count_cols:\n    q1 = data[column].quantile(0.25)\n    q3 = data[column].quantile(0.75)\n    iqr = q3 - q1\n    median = data[column].median()\n    outlier_threshold = median + 1.5*iqr\n\n    # Count the number of values that exceed the outlier threshold\n    outlier_count = (data[column] > outlier_threshold).sum()\n    print(f'Number of outliers, {column}:', outlier_count)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:31:56.727147Z","iopub.execute_input":"2024-08-19T13:31:56.727970Z","iopub.status.idle":"2024-08-19T13:31:56.748950Z","shell.execute_reply.started":"2024-08-19T13:31:56.727939Z","shell.execute_reply":"2024-08-19T13:31:56.747912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.4. Visualizations - Outliers and Variable Distributions**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5,2))\nplt.hist(data['video_duration_sec'], bins=range(0,61,5))\nplt.title('Video Duration Histogram')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:35:44.782498Z","iopub.execute_input":"2024-08-19T13:35:44.782886Z","iopub.status.idle":"2024-08-19T13:35:45.025089Z","shell.execute_reply.started":"2024-08-19T13:35:44.782858Z","shell.execute_reply":"2024-08-19T13:35:45.024199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(7,2))\nplt.hist(data['video_view_count'], bins=range(0,(10**6+1),10**5))\nlabels = [0] + [str(i) + 'k' for i in range(100, 1001, 100)]\nplt.xticks(range(0, 1*10**6+1, 10**5), labels=labels)\nplt.title('Video View Count Histogram')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:36:00.131607Z","iopub.execute_input":"2024-08-19T13:36:00.132220Z","iopub.status.idle":"2024-08-19T13:36:00.314792Z","shell.execute_reply.started":"2024-08-19T13:36:00.132190Z","shell.execute_reply":"2024-08-19T13:36:00.313920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5, 2))\nn, bins, patches = plt.hist(data['video_like_count'], bins=range(0, (7*10**5+1), 10**5))\nlabels = [0] + [str(i) + 'k' for i in range(100, 701, 100)]\nplt.xticks(range(0, 7*10**5+1, 10**5), labels=labels)\nplt.title('Video Like Count Histogram')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:36:13.198306Z","iopub.execute_input":"2024-08-19T13:36:13.199192Z","iopub.status.idle":"2024-08-19T13:36:13.404001Z","shell.execute_reply.started":"2024-08-19T13:36:13.199156Z","shell.execute_reply":"2024-08-19T13:36:13.403149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,2))\nplt.hist(data['video_comment_count'])\nplt.title('Video Comment Count Histogram')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:36:27.252959Z","iopub.execute_input":"2024-08-19T13:36:27.253313Z","iopub.status.idle":"2024-08-19T13:36:27.473387Z","shell.execute_reply.started":"2024-08-19T13:36:27.253284Z","shell.execute_reply":"2024-08-19T13:36:27.472503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,2))\nplt.hist(data['video_share_count'])\nplt.title('Video Share Count Histogram')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:36:35.832926Z","iopub.execute_input":"2024-08-19T13:36:35.833565Z","iopub.status.idle":"2024-08-19T13:36:36.054611Z","shell.execute_reply.started":"2024-08-19T13:36:35.833533Z","shell.execute_reply":"2024-08-19T13:36:36.053673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,2))\nplt.hist(data['video_download_count'])\nplt.title('Video Download Count Histogram')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:36:42.240685Z","iopub.execute_input":"2024-08-19T13:36:42.241272Z","iopub.status.idle":"2024-08-19T13:36:42.470419Z","shell.execute_reply.started":"2024-08-19T13:36:42.241243Z","shell.execute_reply":"2024-08-19T13:36:42.469482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from the graphs, all variables have a distribution skewed to the right, except the `video_duration_sec` variable which has a uniform distribution. This way, we can understand which variables we can use in what type of models.\n\nNow let's create a histogram to see how many videos we have of claims and opinions in terms of verified and unverified users.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nplt.figure(figsize=(7,3))\nsns.histplot(\n    data = data,\n    x = 'claim_status',\n    hue = 'verified_status',\n    multiple='dodge',              # This is so that four separate bars appear, two blues and two oranges.\n    shrink=0.9)\nplt.title('Claims by Verification Status Histogram');","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:06:31.144675Z","iopub.execute_input":"2024-08-19T14:06:31.145888Z","iopub.status.idle":"2024-08-19T14:06:31.478678Z","shell.execute_reply.started":"2024-08-19T14:06:31.145854Z","shell.execute_reply":"2024-08-19T14:06:31.477678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, users that are not verified tend to post videos with claims instead of opinions.\n\nNext, I'm going to create a histogram of the number of videos with claims and opinions by author ban status.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nplt.figure(figsize=(7,3))\nsns.histplot(\n    data = data,\n    x = 'claim_status',\n    hue = 'author_ban_status',\n    multiple='dodge',              \n    shrink=0.9)\nplt.title('Claims by Author Ban Status Histogram')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:06:41.666287Z","iopub.execute_input":"2024-08-19T14:06:41.667081Z","iopub.status.idle":"2024-08-19T14:06:41.975537Z","shell.execute_reply.started":"2024-08-19T14:06:41.667048Z","shell.execute_reply":"2024-08-19T14:06:41.974314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a higher number of users who post videos with claims who are either banned or under review. In other words, users who post videos with claims appear to have a greater likelihood of being banned.\n\nLet's check how many views, on average, does the videos posted by the different author ban status receive.","metadata":{}},{"cell_type":"code","source":"ban_status_median = data.groupby(['author_ban_status']).median(\n    numeric_only=True).reset_index()\n\nfig = plt.figure(figsize=(5,3))\nsns.barplot(data=ban_status_median,\n            x='author_ban_status',\n            y='video_view_count',\n            order=['active', 'under review', 'banned'],\n            palette={'active':'green', 'under review':'orange', 'banned':'red'},\n            alpha=0.5)\nplt.title('Mean Visualization Counts by Ban Status');","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:58:12.343242Z","iopub.execute_input":"2024-08-19T13:58:12.343697Z","iopub.status.idle":"2024-08-19T13:58:12.594901Z","shell.execute_reply.started":"2024-08-19T13:58:12.343668Z","shell.execute_reply":"2024-08-19T13:58:12.593963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can clearly see, videos posted by users that are banned received far more views than those that are active.\n\n**Therefore, the views variable could be a good indicator of videos with claims.**\n\nLet's check this indicator further by seeing the median of views by claim status and plotting a graph with the total views by claim status.","metadata":{}},{"cell_type":"code","source":"data.groupby('claim_status')['video_view_count'].median()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:44:00.561795Z","iopub.execute_input":"2024-08-19T13:44:00.562762Z","iopub.status.idle":"2024-08-19T13:44:00.574792Z","shell.execute_reply.started":"2024-08-19T13:44:00.562728Z","shell.execute_reply":"2024-08-19T13:44:00.573844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(3,3))\nplt.pie(data.groupby('claim_status')['video_view_count'].sum(), labels=['Claims', 'Opinions'])\nplt.title('Total Visualizations by Claim Status');","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:44:03.270648Z","iopub.execute_input":"2024-08-19T13:44:03.271631Z","iopub.status.idle":"2024-08-19T13:44:03.387227Z","shell.execute_reply.started":"2024-08-19T13:44:03.271597Z","shell.execute_reply":"2024-08-19T13:44:03.386003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We can then confirm that videos with claims clearly have many more views than videos with opinions.**","metadata":{}},{"cell_type":"markdown","source":"## **2.5. Checking Missing Values**","metadata":{}},{"cell_type":"code","source":"data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:44:52.426322Z","iopub.execute_input":"2024-08-19T13:44:52.427066Z","iopub.status.idle":"2024-08-19T13:44:52.444563Z","shell.execute_reply.started":"2024-08-19T13:44:52.427028Z","shell.execute_reply":"2024-08-19T13:44:52.443236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.6. Summary and Key Insights of EDA**\n\nAccording to the findings from the exploratory data analysis, the future claim classification model will need to account for null values and imbalance in opinion video engagement counts by incorporating them into the model parameters.\n\nA key component of this project’s exploratory data analysis involves visualizing the data. As illustrated before with the histograms, it is clear that the vast majority of videos are grouped at the bottom of the range of values, which means that there are several videos with extreme counts of views, likes and comments for example.\n\nOver 200 null values were found in 7 different columns. As a result, future modeling should consider the null values to avoid making insights that would assume complete data. Further analysis is necessary to investigate the reason for these null values, and their impact on future statistical analysis or model building.\n\nThe `video_view_count` variable could be a good indicator of videos with claims.","metadata":{}},{"cell_type":"markdown","source":"# **3. Statistical Analysis and Hypothesis testing**\n\nAt this stage, the number 1 question we should ask is, what is the research question we want answered? Later on, I will need to formulate the null and alternative hypotheses as the first step of my hypothesis test. Let's consider our research question now, at the start of this task:\n\n1) Do videos from verified accounts and videos from unverified accounts have different average view counts?\n\n2) Is there a relationship between the account being verified and the associated videos' view counts?\n\n## **3.1. Importing Packages for Statistical Analysis/Hypothesis Testing**","metadata":{}},{"cell_type":"code","source":"from scipy import stats","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:07:35.858041Z","iopub.execute_input":"2024-08-19T14:07:35.858911Z","iopub.status.idle":"2024-08-19T14:07:35.863691Z","shell.execute_reply.started":"2024-08-19T14:07:35.858876Z","shell.execute_reply":"2024-08-19T14:07:35.862523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3.2. Data Cleaning**\n\nWe know that we have some null values so, in this part of the analysis, it's important to eliminate them in order to do the hypothesis test. This is because we have a big dataset, and a few null values won’t make that much of a difference.\n\nAfter that, I'm going to see if there are duplicated rows of data.","metadata":{}},{"cell_type":"code","source":"# Dropping rows with missing values\n\ndata_clean = data.dropna(axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:09:54.410724Z","iopub.execute_input":"2024-08-19T14:09:54.411455Z","iopub.status.idle":"2024-08-19T14:09:54.428259Z","shell.execute_reply.started":"2024-08-19T14:09:54.411421Z","shell.execute_reply":"2024-08-19T14:09:54.427232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking duplicated rows\n\ndata_clean.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:10:08.550824Z","iopub.execute_input":"2024-08-19T14:10:08.551540Z","iopub.status.idle":"2024-08-19T14:10:08.587117Z","shell.execute_reply.started":"2024-08-19T14:10:08.551497Z","shell.execute_reply":"2024-08-19T14:10:08.586087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Computing the mean `video_view_count` for each group in `verified_status`\n\ndata_clean.groupby(['verified_status']).agg({'video_view_count':'mean'})","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:10:46.170095Z","iopub.execute_input":"2024-08-19T14:10:46.170485Z","iopub.status.idle":"2024-08-19T14:10:46.185824Z","shell.execute_reply.started":"2024-08-19T14:10:46.170439Z","shell.execute_reply":"2024-08-19T14:10:46.184619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3.3. Hypothesis Testing**\n\n*   **Null hypothesis $H_0$**: There is no difference in number of views between TikTok videos posted by verified accounts and TikTok videos posted by unverified accounts (any observed difference in the sample data is due to chance or sampling variability).\n*    **Alternative hypothesis $H_A$**: There is a difference in number of views between TikTok videos posted by verified accounts and TikTok videos posted by unverified accounts (any observed difference in the sample data is due to an actual difference in the corresponding population means).\n\nI'm going to choose 5% as the significance level and proceed with a two-sample t-test.","metadata":{}},{"cell_type":"code","source":"# Conducting a two-sample t-test to compare means\n\nnot_verified = data_clean[data_clean['verified_status'] == 'not verified']\n\nverified = data_clean[data_clean['verified_status'] == 'verified']\n\nstats.ttest_ind(a = not_verified['video_view_count'], b = verified['video_view_count'], equal_var=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:12:14.615933Z","iopub.execute_input":"2024-08-19T14:12:14.616639Z","iopub.status.idle":"2024-08-19T14:12:14.640882Z","shell.execute_reply.started":"2024-08-19T14:12:14.616605Z","shell.execute_reply":"2024-08-19T14:12:14.639901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Based on the p-value we got above, do we reject or fail to reject the null hypothesis?**\n\n* We concluded that our p-value is very small, therefore:\n\n* **P-value < 5%, then we reject the Null Hypothesis and conclude that there is a statistically significant difference in the average view counts between videos from verified accounts and videos from unverified accounts.**","metadata":{}},{"cell_type":"markdown","source":"## **3.4. Summary and Key Insights of Hypothesis Testing**\n\nI've considered the relationship between `verified_status` and `video_view_count`. \nOne approach conducted was to examine the mean values of `video_view_count` for each group of `verified_status` in the sample data. The findings showed that unverified accounts have a mean of 265,663 views vs. 91,439 views for verified accounts.\n\nThe second approach was a two-sample hypothesis test. Aligned with preliminary findings from the mean values, this statistical analysis shows that any observed difference in the sample data is due to an actual difference in the corresponding population means.\n\nThe analysis shows that there is a statistically significant difference in the average view counts between videos from verified accounts and videos from unverified accounts. This suggests there might be fundamental behavioral differences between these two groups of accounts.\n\nIt would be interesting to investigate the root cause of this behavioral difference. For example, do unverified accounts tend to post more clickbait-y videos? Or are unverified accounts associated with spam bots that help inflate view counts?\n\nThe next step will be to build a regression model on `verified_status`. A regression model is the natural next step because the end goal is to make predictions on `claim_status`. A regression model for `verified_status` can help analyze user behavior in this group of verified users. Technical note to prepare regression model: because the data is skewed, and there is a significant difference in account types, it will be key to build a **logistic regression model**.","metadata":{}},{"cell_type":"markdown","source":"# **4. Regression Analysis**\n\nLogistic regression helps me estimate the probability of an outcome. For data science professionals, this is a useful skill because it allows us to consider more than one variable against the variable we're measuring against. This opens the door for much more thorough and flexible analysis to be completed.\n\nI'm interested in how different variables are associated with whether a user is verified. Earlier, I've observed that if a user is verified, they are much more likely to post opinions. Now, I've decided to explore how to predict verified status to help me understand how video characteristics relate to verified users. Therefore, I'm going to conduct a **logistic regression** using `verified_status` as the outcome variable. The results may be used to inform the final model related to predicting whether a video is a claim or an opinion.\n\n## **4.1. Importing Packages for Data Preprocessing and Data Modeling**","metadata":{}},{"cell_type":"code","source":"# Importing packages for data preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.utils import resample\n\n# Importing packages for data modeling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:25:01.699016Z","iopub.execute_input":"2024-08-19T14:25:01.699867Z","iopub.status.idle":"2024-08-19T14:25:02.105729Z","shell.execute_reply.started":"2024-08-19T14:25:01.699831Z","shell.execute_reply":"2024-08-19T14:25:02.104747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **4.2. EDA & Checking Model Assumptions**\n\nFor me to perform a logistic regression analysis, we must understand whether our data is in accordance with 4 assumptions: **linearity**, **independent observations**, **no outliers**, and **non-multicollinearity**.\n\n### 4.2.1. Independent Observations\nI know that each row represents a different video, so the **independent observations** assumption is already met.\n\n### 4.2.2. Outliers\nAs you know, we have several outliers in various variables so, we have to deal we them for this part of the project.","metadata":{}},{"cell_type":"code","source":"# Handling outliers for 'video_like_count'\n\npercentile25 = data_clean[\"video_like_count\"].quantile(0.25)\npercentile75 = data_clean[\"video_like_count\"].quantile(0.75)\n\niqr = percentile75 - percentile25\nupper_limit = percentile75 + 1.5 * iqr\n\ndata_clean.loc[data_clean[\"video_like_count\"] > upper_limit, \"video_like_count\"] = upper_limit # replaces values that are greater than the upper limit with the upper limit itself","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:26:19.768960Z","iopub.execute_input":"2024-08-19T14:26:19.769724Z","iopub.status.idle":"2024-08-19T14:26:19.780322Z","shell.execute_reply.started":"2024-08-19T14:26:19.769690Z","shell.execute_reply":"2024-08-19T14:26:19.779282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handling outliers for 'video_comment_count'\n\npercentile25 = data_clean[\"video_comment_count\"].quantile(0.25)\npercentile75 = data_clean[\"video_comment_count\"].quantile(0.75)\n\niqr = percentile75 - percentile25\nupper_limit = percentile75 + 1.5 * iqr\n\ndata_clean.loc[data_clean[\"video_comment_count\"] > upper_limit, \"video_comment_count\"] = upper_limit # replaces values that are greater than the upper limit with the upper limit itself","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:26:26.824070Z","iopub.execute_input":"2024-08-19T14:26:26.824989Z","iopub.status.idle":"2024-08-19T14:26:26.834494Z","shell.execute_reply.started":"2024-08-19T14:26:26.824953Z","shell.execute_reply":"2024-08-19T14:26:26.833545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handle outliers for 'video_share_count'\n\npercentile25 = data_clean[\"video_share_count\"].quantile(0.25)\npercentile75 = data_clean[\"video_share_count\"].quantile(0.75)\n\niqr = percentile75 - percentile25\nupper_limit = percentile75 + 1.5 * iqr\n\ndata_clean.loc[data_clean[\"video_share_count\"] > upper_limit, \"video_share_count\"] = upper_limit # replaces values that are greater than the upper limit with the upper limit itself","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:26:31.384891Z","iopub.execute_input":"2024-08-19T14:26:31.385274Z","iopub.status.idle":"2024-08-19T14:26:31.395445Z","shell.execute_reply.started":"2024-08-19T14:26:31.385243Z","shell.execute_reply":"2024-08-19T14:26:31.394530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handle outliers for 'video_download_count'\n\npercentile25 = data_clean[\"video_download_count\"].quantile(0.25)\npercentile75 = data_clean[\"video_download_count\"].quantile(0.75)\n\niqr = percentile75 - percentile25\nupper_limit = percentile75 + 1.5 * iqr\n\ndata_clean.loc[data_clean[\"video_download_count\"] > upper_limit, \"video_download_count\"] = upper_limit # replaces values that are greater than the upper limit with the upper limit itself","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:26:38.842477Z","iopub.execute_input":"2024-08-19T14:26:38.843432Z","iopub.status.idle":"2024-08-19T14:26:38.853527Z","shell.execute_reply.started":"2024-08-19T14:26:38.843394Z","shell.execute_reply":"2024-08-19T14:26:38.852488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking class balance for 'verified_status'\n\ndata_clean[\"verified_status\"].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:27:28.874629Z","iopub.execute_input":"2024-08-19T14:27:28.875040Z","iopub.status.idle":"2024-08-19T14:27:28.887326Z","shell.execute_reply.started":"2024-08-19T14:27:28.875007Z","shell.execute_reply":"2024-08-19T14:27:28.886204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Approximately 93.7% of the dataset represents videos posted by unverified accounts and 6.2% represents videos posted by verified accounts. So, the outcome variable is not very balanced.\n\nI'm going to use resampling to create class balance in the outcome variable.","metadata":{}},{"cell_type":"code","source":"# Identifying data points from majority and minority classes\ndata_majority = data_clean[data_clean[\"verified_status\"] == \"not verified\"]\ndata_minority = data_clean[data_clean[\"verified_status\"] == \"verified\"]\n\n# Upsampling the minority class (which is \"verified\")\ndata_minority_upsampled = resample(data_minority,\n                                 replace=True,                 # to sample with replacement\n                                 n_samples=len(data_majority), # to match majority class\n                                 random_state=42)               # to create reproducible results\n\n# Combining majority class with upsampled minority class\ndata_upsampled = pd.concat([data_majority, data_minority_upsampled]).reset_index(drop=True)\n\n# Checking class balance\ndata_upsampled[\"verified_status\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:30:28.682763Z","iopub.execute_input":"2024-08-19T14:30:28.683698Z","iopub.status.idle":"2024-08-19T14:30:28.725861Z","shell.execute_reply.started":"2024-08-19T14:30:28.683666Z","shell.execute_reply":"2024-08-19T14:30:28.724983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I'm going to get the average `video_transcription_text` length for videos posted by verified accounts and the average `video_transcription_text` length for videos posted by unverified accounts.\n\nThen I'm going to extract the length of each `video_transcription_text` and add this as a column to the data frame, so that it can be used as a potential feature in the model.","metadata":{}},{"cell_type":"code","source":"data_upsampled[[\"verified_status\", \"video_transcription_text\"]].groupby(by=\"verified_status\")[[\"video_transcription_text\"]].agg(func=lambda array: np.mean([len(text) for text in array]))","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:31:55.384647Z","iopub.execute_input":"2024-08-19T14:31:55.385597Z","iopub.status.idle":"2024-08-19T14:31:55.423353Z","shell.execute_reply.started":"2024-08-19T14:31:55.385555Z","shell.execute_reply":"2024-08-19T14:31:55.422387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extractin the length of each `video_transcription_text` and adding this as a column to the dataframe\n\ndata_upsampled['text_length'] = data_upsampled[\"video_transcription_text\"].apply(func=lambda text: len(text))","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:32:02.119493Z","iopub.execute_input":"2024-08-19T14:32:02.120324Z","iopub.status.idle":"2024-08-19T14:32:02.155560Z","shell.execute_reply.started":"2024-08-19T14:32:02.120274Z","shell.execute_reply":"2024-08-19T14:32:02.154481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_upsampled.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:32:12.521571Z","iopub.execute_input":"2024-08-19T14:32:12.521958Z","iopub.status.idle":"2024-08-19T14:32:12.543514Z","shell.execute_reply.started":"2024-08-19T14:32:12.521928Z","shell.execute_reply":"2024-08-19T14:32:12.542416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**I want to visualize the distribution of `video_transcription_text` length for videos posted by verified accounts and videos posted by unverified accounts.**","metadata":{}},{"cell_type":"code","source":"verified = data_upsampled['text_length'][data_upsampled['verified_status']=='verified']\nunverified = data_upsampled['text_length'][data_upsampled['verified_status']=='not verified']\n\nfig, axes = plt.subplots(1, 2, figsize = (10,4))\n\nsns.histplot(verified, ax = axes[0])\naxes[0].set_xlabel('Text Length')\naxes[0].set_title('Text lenght on Verified Users Histogram')\n\nsns.histplot(unverified, ax = axes[1])\naxes[1].set_xlabel('Text Length')\naxes[1].set_title('Text lenght on Unverified Users Histogram')","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:33:11.291031Z","iopub.execute_input":"2024-08-19T14:33:11.291493Z","iopub.status.idle":"2024-08-19T14:33:12.126548Z","shell.execute_reply.started":"2024-08-19T14:33:11.291436Z","shell.execute_reply":"2024-08-19T14:33:12.125526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Same information, different plot\n\nsns.histplot(data=data_upsampled, stat=\"count\", multiple=\"dodge\", x=\"text_length\",\n             kde=False, palette=\"pastel\", hue=\"claim_status\",\n             element=\"bars\", legend=True)\nplt.xlabel(\"video_transcription_text length (number of characters)\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of video_transcription_text length for claims and opinions\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:34:13.368120Z","iopub.execute_input":"2024-08-19T14:34:13.369167Z","iopub.status.idle":"2024-08-19T14:34:14.089647Z","shell.execute_reply.started":"2024-08-19T14:34:13.369122Z","shell.execute_reply":"2024-08-19T14:34:14.088623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.3. Non-multicollinearity - Examining Correlations\n\nNext, I'm going to code a correlation matrix to help determine most correlated variables and create a heatmap to visualize these correlations.","metadata":{}},{"cell_type":"code","source":"data_upsampled.corr(numeric_only=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:47:32.852403Z","iopub.execute_input":"2024-08-19T14:47:32.853153Z","iopub.status.idle":"2024-08-19T14:47:32.890459Z","shell.execute_reply.started":"2024-08-19T14:47:32.853107Z","shell.execute_reply":"2024-08-19T14:47:32.889424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.heatmap(\n    data_upsampled[[\"video_duration_sec\", \"claim_status\", \"author_ban_status\", \"video_view_count\", \n                    \"video_like_count\", \"video_share_count\", \"video_download_count\", \"video_comment_count\", \"text_length\"]]\n    .corr(numeric_only=True), \n    annot=True, \n    cmap=\"crest\")\nplt.title(\"Heatmap of the dataset\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:47:42.223175Z","iopub.execute_input":"2024-08-19T14:47:42.223569Z","iopub.status.idle":"2024-08-19T14:47:42.656576Z","shell.execute_reply.started":"2024-08-19T14:47:42.223538Z","shell.execute_reply":"2024-08-19T14:47:42.655458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above heatmap shows that the following pair of variables are strongly correlated: `video_view_count` and `video_like_count` (0.86 correlation coefficient).\n\nOne of the model assumptions for logistic regression is **no severe multicollinearity** among the features. To build a logistic regression model that meets this assumption, we could exclude `video_like_count`. And among the variables that quantify video metrics, we could keep `video_view_count`, `video_share_count`, `video_download_count`, and `video_comment_count` as features.","metadata":{}},{"cell_type":"markdown","source":"## **4.3. Constructing the Logistic Regression Model**","metadata":{}},{"cell_type":"code","source":"# Selecting the outcome variable\n\ny = data_upsampled[\"verified_status\"]","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:58:24.624826Z","iopub.execute_input":"2024-08-19T14:58:24.625229Z","iopub.status.idle":"2024-08-19T14:58:24.629921Z","shell.execute_reply.started":"2024-08-19T14:58:24.625198Z","shell.execute_reply":"2024-08-19T14:58:24.628930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting features\nX = data_upsampled[[\"video_duration_sec\", \"claim_status\", \"author_ban_status\", \"video_view_count\", \"video_share_count\", \"video_download_count\", \"video_comment_count\"]]\n\n# Display first few rows of features dataframe\nX.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:58:42.162943Z","iopub.execute_input":"2024-08-19T14:58:42.163805Z","iopub.status.idle":"2024-08-19T14:58:42.181199Z","shell.execute_reply.started":"2024-08-19T14:58:42.163773Z","shell.execute_reply":"2024-08-19T14:58:42.180340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Note: The `#` and `video_id` columns are not selected as features here, because they do not seem to be helpful for predicting whether a video presents a claim or an opinion. Also, `video_like_count` is not selected as a feature here, because it is strongly correlated with other features, as discussed earlier. And logistic regression has a no multicollinearity model assumption that needs to be met.*","metadata":{}},{"cell_type":"code","source":"# Splitting the data into training and testing sets\n\nX_train, X_test, y_train, y_test = train_test_split (X, y, test_size = 0.25, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:00:02.125002Z","iopub.execute_input":"2024-08-19T15:00:02.125436Z","iopub.status.idle":"2024-08-19T15:00:02.141718Z","shell.execute_reply.started":"2024-08-19T15:00:02.125404Z","shell.execute_reply":"2024-08-19T15:00:02.140850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting shape of each training and testing set to confirm that the dimensions are in alignment.\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:00:20.622216Z","iopub.execute_input":"2024-08-19T15:00:20.623011Z","iopub.status.idle":"2024-08-19T15:00:20.629370Z","shell.execute_reply.started":"2024-08-19T15:00:20.622981Z","shell.execute_reply":"2024-08-19T15:00:20.628356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The number of features (7) aligns between the training and testing sets.\n* The number of rows aligns between the features and the outcome variable for training (26826) and testing (8942).","metadata":{}},{"cell_type":"code","source":"# Checking data types\n\nX_train.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:01:08.983432Z","iopub.execute_input":"2024-08-19T15:01:08.984345Z","iopub.status.idle":"2024-08-19T15:01:08.991548Z","shell.execute_reply.started":"2024-08-19T15:01:08.984315Z","shell.execute_reply":"2024-08-19T15:01:08.990590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As shown above, the `claim_status` and `author_ban_status` features are each of data type object currently. In order to work with the implementations of models through sklearn, these categorical features will need to be made numeric. One way to do this is through one-hot encoding.","metadata":{}},{"cell_type":"code","source":"# Selecting the training features that needs to be encoded\nX_train_to_encode = X_train[[\"claim_status\", \"author_ban_status\"]]\n\n# Display first few rows\nX_train_to_encode.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:02:04.239761Z","iopub.execute_input":"2024-08-19T15:02:04.240539Z","iopub.status.idle":"2024-08-19T15:02:04.251996Z","shell.execute_reply.started":"2024-08-19T15:02:04.240507Z","shell.execute_reply":"2024-08-19T15:02:04.250914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up an encoder for one-hot encoding the categorical features\nX_encoder = OneHotEncoder(drop='first', sparse_output=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:02:19.752998Z","iopub.execute_input":"2024-08-19T15:02:19.753853Z","iopub.status.idle":"2024-08-19T15:02:19.758495Z","shell.execute_reply.started":"2024-08-19T15:02:19.753823Z","shell.execute_reply":"2024-08-19T15:02:19.757447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting and transforming the training features using the encoder\nX_train_encoded = X_encoder.fit_transform(X_train_to_encode)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:02:44.511923Z","iopub.execute_input":"2024-08-19T15:02:44.512691Z","iopub.status.idle":"2024-08-19T15:02:44.545793Z","shell.execute_reply.started":"2024-08-19T15:02:44.512660Z","shell.execute_reply":"2024-08-19T15:02:44.544872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting feature names from encoder\nX_encoder.get_feature_names_out()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:02:48.586890Z","iopub.execute_input":"2024-08-19T15:02:48.587601Z","iopub.status.idle":"2024-08-19T15:02:48.593793Z","shell.execute_reply.started":"2024-08-19T15:02:48.587570Z","shell.execute_reply":"2024-08-19T15:02:48.592835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Placing encoded training features (which are currently an array) into a dataframe\nX_train_encoded_df = pd.DataFrame(data=X_train_encoded, columns=X_encoder.get_feature_names_out())\n\n# Display first few rows\nX_train_encoded_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:03:11.761806Z","iopub.execute_input":"2024-08-19T15:03:11.762768Z","iopub.status.idle":"2024-08-19T15:03:11.777278Z","shell.execute_reply.started":"2024-08-19T15:03:11.762730Z","shell.execute_reply":"2024-08-19T15:03:11.776262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display first few rows of `X_train` with `claim_status` and `author_ban_status` columns dropped (since these features are being transformed to numeric)\nX_train.drop(columns=[\"claim_status\", \"author_ban_status\"]).head()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:03:25.159808Z","iopub.execute_input":"2024-08-19T15:03:25.160917Z","iopub.status.idle":"2024-08-19T15:03:25.175906Z","shell.execute_reply.started":"2024-08-19T15:03:25.160884Z","shell.execute_reply":"2024-08-19T15:03:25.174701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenating `X_train` and `X_train_encoded_df` to form the final dataframe for training data (`X_train_final`)\n# Note: Using `.reset_index(drop=True)` to reset the index in X_train after dropping `claim_status` and `author_ban_status`,\n# so that the indices align with those in `X_train_encoded_df` and `count_df`\nX_train_final = pd.concat([X_train.drop(columns=[\"claim_status\", \"author_ban_status\"]).reset_index(drop=True), X_train_encoded_df], axis=1)\n\n# Display first few rows\nX_train_final.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:03:30.999226Z","iopub.execute_input":"2024-08-19T15:03:30.999739Z","iopub.status.idle":"2024-08-19T15:03:31.023327Z","shell.execute_reply.started":"2024-08-19T15:03:30.999706Z","shell.execute_reply":"2024-08-19T15:03:31.022437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now, on to checking the outcome variable**","metadata":{}},{"cell_type":"code","source":"# Checking data type of outcome variable\ny_train.dtype","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:03:53.799112Z","iopub.execute_input":"2024-08-19T15:03:53.799918Z","iopub.status.idle":"2024-08-19T15:03:53.805946Z","shell.execute_reply.started":"2024-08-19T15:03:53.799886Z","shell.execute_reply":"2024-08-19T15:03:53.805028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting unique values of outcome variable\ny_train.unique()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:03:56.150914Z","iopub.execute_input":"2024-08-19T15:03:56.151328Z","iopub.status.idle":"2024-08-19T15:03:56.160754Z","shell.execute_reply.started":"2024-08-19T15:03:56.151295Z","shell.execute_reply":"2024-08-19T15:03:56.159680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**A shown above, the outcome variable is of data type object currently. One-hot encoding can be used to make this variable numeric.**","metadata":{}},{"cell_type":"code","source":"# Setting up an encoder for one-hot encoding the categorical outcome variable\ny_encoder = OneHotEncoder(drop='first', sparse_output=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:04:21.624976Z","iopub.execute_input":"2024-08-19T15:04:21.625758Z","iopub.status.idle":"2024-08-19T15:04:21.630711Z","shell.execute_reply.started":"2024-08-19T15:04:21.625717Z","shell.execute_reply":"2024-08-19T15:04:21.629504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding the training outcome variable\n# Notes:\n#   - Adjusting the shape of `y_train` before passing into `.fit_transform()`, since it takes in 2D array\n#   - Using `.ravel()` to flatten the array returned by `.fit_transform()`, so that it can be used later to train the model\ny_train_final = y_encoder.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n\n# Display the encoded training outcome variable\ny_train_final","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:04:24.182616Z","iopub.execute_input":"2024-08-19T15:04:24.182982Z","iopub.status.idle":"2024-08-19T15:04:24.205028Z","shell.execute_reply.started":"2024-08-19T15:04:24.182954Z","shell.execute_reply":"2024-08-19T15:04:24.203896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now, onto constructing the actual model.**","metadata":{}},{"cell_type":"code","source":"# Constructing a logistic regression model and fitting it to the training set\nclf = LogisticRegression(random_state=42, max_iter=800).fit(X_train_final, y_train_final)\n\n# max_iter = 800 is the maximum number of iterations that the optimization algorithm should perform.\n# This is useful to ensure that the model has time to converge.","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:04:41.420617Z","iopub.execute_input":"2024-08-19T15:04:41.421336Z","iopub.status.idle":"2024-08-19T15:04:41.541915Z","shell.execute_reply.started":"2024-08-19T15:04:41.421305Z","shell.execute_reply":"2024-08-19T15:04:41.540458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **4.4. Results and Evaluation**\n\n### 4.4.1. Encoding categorical features in the testing set","metadata":{}},{"cell_type":"code","source":"# Selecting the testing features that needs to be encoded\nX_test_to_encode = X_test[['claim_status', 'author_ban_status']]\n\n# Display first few rows\nX_test_to_encode.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:05:14.936194Z","iopub.execute_input":"2024-08-19T15:05:14.936611Z","iopub.status.idle":"2024-08-19T15:05:14.948781Z","shell.execute_reply.started":"2024-08-19T15:05:14.936581Z","shell.execute_reply":"2024-08-19T15:05:14.947651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transforming the testing features using the encoder\nX_test_encoded = X_encoder.fit_transform(X_test_to_encode)\n\n# Display first few rows of encoded testing features\nX_test_encoded","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:05:21.392392Z","iopub.execute_input":"2024-08-19T15:05:21.392804Z","iopub.status.idle":"2024-08-19T15:05:21.411707Z","shell.execute_reply.started":"2024-08-19T15:05:21.392774Z","shell.execute_reply":"2024-08-19T15:05:21.410445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Placing encoded testing features (which is currently an array) into a dataframe\nX_test_encoded_df = pd.DataFrame(data=X_test_encoded, columns=X_encoder.get_feature_names_out())\n\n# Display first few rows\nX_test_encoded_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:05:24.181417Z","iopub.execute_input":"2024-08-19T15:05:24.182562Z","iopub.status.idle":"2024-08-19T15:05:24.196095Z","shell.execute_reply.started":"2024-08-19T15:05:24.182524Z","shell.execute_reply":"2024-08-19T15:05:24.194922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display first few rows of `X_test` with `claim_status` and `author_ban_status` columns dropped (since these features are being transformed to numeric)\nX_test.drop(columns=[\"claim_status\", \"author_ban_status\"]).head()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:05:27.512593Z","iopub.execute_input":"2024-08-19T15:05:27.513603Z","iopub.status.idle":"2024-08-19T15:05:27.529695Z","shell.execute_reply.started":"2024-08-19T15:05:27.513565Z","shell.execute_reply":"2024-08-19T15:05:27.528526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenating `X_test` and `X_test_encoded_df` to form the final dataframe for training data (`X_test_final`)\n# Note: Using `.reset_index(drop=True)` to reset the index in X_test after dropping `claim_status`, and `author_ban_status`,\n# so that the indices align with those in `X_test_encoded_df` and `test_count_df`\nX_test_final = pd.concat([X_test.drop(columns=[\"claim_status\", \"author_ban_status\"]).reset_index(drop=True), X_test_encoded_df], axis=1)\n\n# Display first few rows\nX_test_final.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:05:30.740781Z","iopub.execute_input":"2024-08-19T15:05:30.741495Z","iopub.status.idle":"2024-08-19T15:05:30.762689Z","shell.execute_reply.started":"2024-08-19T15:05:30.741450Z","shell.execute_reply":"2024-08-19T15:05:30.761284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.4.2. Results and Evaluation","metadata":{}},{"cell_type":"code","source":"# Use the logistic regression model to get predictions on the encoded testing set\ny_pred = clf.predict(X_test_final)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:05:46.650794Z","iopub.execute_input":"2024-08-19T15:05:46.651223Z","iopub.status.idle":"2024-08-19T15:05:46.662026Z","shell.execute_reply.started":"2024-08-19T15:05:46.651191Z","shell.execute_reply":"2024-08-19T15:05:46.660563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode the testing outcome variable\n# Notes:\n#   - Adjusting the shape of `y_test` before passing into `.transform()`, since it takes in 2D array\n#   - Using `.ravel()` to flatten the array returned by `.transform()`, so that it can be used later to compare with predictions\ny_test_final = y_encoder.fit_transform(y_test.values.reshape(-1, 1)).ravel()\n\n# Display the encoded testing outcome variable\ny_test_final","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:05:49.175585Z","iopub.execute_input":"2024-08-19T15:05:49.175962Z","iopub.status.idle":"2024-08-19T15:05:49.191586Z","shell.execute_reply.started":"2024-08-19T15:05:49.175935Z","shell.execute_reply":"2024-08-19T15:05:49.190499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get shape of each training and testing set\nX_train_final.shape, X_test_final.shape, y_train_final.shape, y_test_final.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:05:51.284669Z","iopub.execute_input":"2024-08-19T15:05:51.285531Z","iopub.status.idle":"2024-08-19T15:05:51.292693Z","shell.execute_reply.started":"2024-08-19T15:05:51.285497Z","shell.execute_reply":"2024-08-19T15:05:51.291542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute values for confusion matrix\ncm = confusion_matrix(y_test_final, y_pred, labels = clf.classes_)\n\n# Create display of confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = clf.classes_)\n\n# Plot confusion matrix\ndisp.plot()\n\n# Display plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:05:53.812094Z","iopub.execute_input":"2024-08-19T15:05:53.812512Z","iopub.status.idle":"2024-08-19T15:05:54.128621Z","shell.execute_reply.started":"2024-08-19T15:05:53.812482Z","shell.execute_reply":"2024-08-19T15:05:54.127375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(3856+1991) / (3856 + 649 + 2446 + 1991)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:05:57.794377Z","iopub.execute_input":"2024-08-19T15:05:57.795130Z","iopub.status.idle":"2024-08-19T15:05:57.802863Z","shell.execute_reply.started":"2024-08-19T15:05:57.795087Z","shell.execute_reply":"2024-08-19T15:05:57.801567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The upper-left quadrant displays the number of **true negatives**: the number of videos posted by unverified accounts that the model accurately classified as so.\n\n* The upper-right quadrant displays the number of **false positives**: the number of videos posted by unverified accounts that the model misclassified as posted by verified accounts.\n\n* The lower-left quadrant displays the number of **false negatives**: the number of videos posted by verified accounts that the model misclassified as posted by unverified accounts.\n\n* The lower-right quadrant displays the number of **true positives**: the number of videos posted by verified accounts that the model accurately classified as so.\n\n* A perfect model would yield all true negatives and true positives, and no false negatives or false positives.","metadata":{}},{"cell_type":"code","source":"# Creating a classification report for logistic regression model\ntarget_labels = [\"verified\", \"not verified\"]\nprint(classification_report(y_test_final, y_pred, target_names=target_labels))","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:07:38.736422Z","iopub.execute_input":"2024-08-19T15:07:38.736876Z","iopub.status.idle":"2024-08-19T15:07:38.770201Z","shell.execute_reply.started":"2024-08-19T15:07:38.736844Z","shell.execute_reply":"2024-08-19T15:07:38.769055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The classification report above shows that the logistic regression model achieved a precision of 62% and a recall of 8%, and it achieved an accuracy of 51%. Note that the precision and recall scores are taken from the \"not verified\" row of the output because that is the target class that we are most interested in predicting. The \"verified\" class has its own precision/recall metrics, and the weighted average represents the combined metrics for both classes of the target variable.\n\n### 4.4.3. Interpreting model coefficients","metadata":{}},{"cell_type":"code","source":"# Get the feature names from the model and the model coefficients (which represent log-odds ratios)\n# Place into a DataFrame for readability\n\npd.DataFrame(data={\"Feature Name\":clf.feature_names_in_, \"Model Coefficient\":clf.coef_[0]})","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:09:54.481211Z","iopub.execute_input":"2024-08-19T15:09:54.481758Z","iopub.status.idle":"2024-08-19T15:09:54.494844Z","shell.execute_reply.started":"2024-08-19T15:09:54.481725Z","shell.execute_reply":"2024-08-19T15:09:54.493553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **4.5. Summary and Key Insights**\n\nThe variable of `verified_status` was selected for this regression model because of the relationship seen between the verified account type and the video content. A logistic regression model was selected because of the data type and distribution.\n\nThe logistic regression model achieved a precision of 56% and a recall of 51% (weighted averages). This model achieved an f1 accuracy of 51%. These model results inform key insights on video features, discussed above:\n\nKey takeaways:\n\n- The dataset has a few strongly correlated variables, which might lead to multicollinearity issues when fitting a logistic regression model. I decided to drop `video_like_count` from the model building.\n- Based on the logistic regression model, each additional second of the video is associated with 0.00004% increase in the log-odds of the user having a verified status.\n- The logistic regression model had not great, but acceptable predictive power: a precision of 56% is less than ideal, but a recall of 51% is not good. Overall accuracy is towards the lower end of what would typically be considered acceptable.\n- Based on the estimated model coefficients from the logistic regression, longer videos tend to be associated with higher odds of the user being verified.\n- Other video features have small estimated coefficients in the model, so their association with verified status seems to be small. As a result, other video features besides video length do not seem to be associated with verified status.\n\nI developed a logistic regression model for verified status based on video features. The model had decent predictive power. Based on the estimated model coefficients from the logistic regression, longer videos tend to be associated with higher odds of the user being verified. Other video features have small estimated coefficients in the model, so their association with verified status seems to be small.\n\nThe next step is to construct a classification model that will predict the status of claims made by users. That is the final project and original expectation from the TikTok team. Now, there is enough information to analyze the results of that model with helpful context around user behavior.","metadata":{}},{"cell_type":"markdown","source":"# **5. Building a Machine Learning Model**\n\nLet's recall some things:\n\n**Business need and modeling objective**\n\nTikTok users can report videos that they believe violate the platform's terms of service. Because there are millions of TikTok videos created and viewed every day, this means that many videos get reported, too many to be individually reviewed by a human moderator.\n\nAnalysis indicates that when authors do violate the terms of service, they're much more likely to be presenting a claim than an opinion. Therefore, it is useful to be able to determine which videos make claims and which videos are opinions.\n\nTikTok wants to build a machine learning model to help identify claims and opinions. Videos that are labeled opinions will be less likely to go on to be reviewed by a human moderator. Videos that are labeled as claims will be further sorted by a downstream process to determine whether they should get prioritized for review. For example, perhaps videos that are classified as claims would then be ranked by how many times they were reported, then the top x% would be reviewed by a human each day.\n\nA machine learning model would greatly assist in the effort to present human moderators with videos that are most likely to be in violation of TikTok's terms of service.\n\n**Modeling design and target variable**\n\nThe data dictionary shows that there is a column called `claim_status`. This is a binary value that indicates whether a video is a claim or an opinion. This will be the target variable. In other words, for each video, the model should predict whether the video is a claim or an opinion.\n\nThis is a classification task because the model is predicting a binary class.\n\n**Selecting an evaluation metric**\n\nTo determine which evaluation metric might be best, we must consider how the model might be wrong. There are two possibilities for bad predictions:\n\n  - **False positives:** When the model predicts a video is a claim when in fact it is an opinion\n  - **False negatives:** When the model predicts a video is an opinion when in fact it is a claim\n\n**2. What are the ethical implications of building the model?**\n\nIn the given scenario, it's better for the model to predict false positives when it makes a mistake, and worse for it to predict false negatives. It's very important to identify videos that break the terms of service, even if that means some opinion videos are misclassified as claims. The worst case for an opinion misclassified as a claim is that the video goes to human review. The worst case for a claim that's misclassified as an opinion is that the video does not get reviewed _and_ it violates the terms of service. A video that violates the terms of service would be considered posted from a \"banned\" author, as referenced in the data dictionary.\n\nBecause it's more important to minimize false negatives, the model evaluation metric will be **recall**.\n\n**Modeling workflow and model selection process**\n\nPrevious work with this data has revealed that there are ~20,000 videos in the sample. This is sufficient to conduct a rigorous model validation workflow, broken into the following steps:\n\n1. Split the data into train/validation/test sets (60/20/20)\n2. Fit models and tune hyperparameters on the training set\n3. Perform final model selection on the validation set\n4. Assess the champion model's performance on the test set\n\n![](https://raw.githubusercontent.com/adacert/tiktok/main/optimal_model_flow_numbered.svg)","metadata":{}},{"cell_type":"markdown","source":"## **5.1. Importing Packages**","metadata":{}},{"cell_type":"code","source":"# Import packages for data preprocessing\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Import packages for data modeling\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, \\\nrecall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:24:12.291733Z","iopub.execute_input":"2024-08-19T15:24:12.292157Z","iopub.status.idle":"2024-08-19T15:24:12.611366Z","shell.execute_reply.started":"2024-08-19T15:24:12.292126Z","shell.execute_reply":"2024-08-19T15:24:12.610387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I've already performed some EDA on this data and cleaned the null values. I've also checked for duplicates and there are no duplicates in this dataset.\n\nAs for the outliers, tree-based models are robust to outliers, so there is no need to impute or drop any values based on where they fall in their distribution. Plus, having these is very helpful in terms of knowing if a video is a claim or opinion.\n\nI've already checked for class balance, and I know that the amount of videos with claims and opinions is very similar, there's no need to adjust anything.\n\n## **5.2. Feature Engineering**\n\nWe've already made some feature engineering when we extracted the length of the text from the videos and created 3 new features: `likes_per_view`, `comments_per_view`, `shares_per_view`.\n\n### 5.2.1. Feature Selection and Transformation\n\nEncoding target and categorical variables.","metadata":{}},{"cell_type":"code","source":"X = data_clean.copy()\n\n# Dropping unnecessary columns\nX = X.drop(['#', 'video_id', 'video_transcription_text'], axis=1)\n\n# Encoding target variable\nX['claim_status'] = X['claim_status'].replace({'opinion': 0, 'claim': 1})\n\n# Dummy encoding remaining categorical values\nX = pd.get_dummies(X,\n                   columns=['verified_status', 'author_ban_status'],\n                   drop_first=True)\nX.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:46:22.715631Z","iopub.execute_input":"2024-08-19T15:46:22.716830Z","iopub.status.idle":"2024-08-19T15:46:22.773634Z","shell.execute_reply.started":"2024-08-19T15:46:22.716780Z","shell.execute_reply":"2024-08-19T15:46:22.772533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **5.3. Splitting the data**\n\nIn this case, the target variable is `claim_status`.\n\n0 represents an opinion\n1 represents a claim","metadata":{}},{"cell_type":"code","source":"# Isolating target variable\ny = X['claim_status']","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:46:26.582169Z","iopub.execute_input":"2024-08-19T15:46:26.582752Z","iopub.status.idle":"2024-08-19T15:46:26.589051Z","shell.execute_reply.started":"2024-08-19T15:46:26.582715Z","shell.execute_reply":"2024-08-19T15:46:26.587920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Isolating features\nX = X.drop(['claim_status'], axis=1)\n\n# Display first few rows of features dataframe\nX.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:46:28.098390Z","iopub.execute_input":"2024-08-19T15:46:28.099349Z","iopub.status.idle":"2024-08-19T15:46:28.122913Z","shell.execute_reply.started":"2024-08-19T15:46:28.099307Z","shell.execute_reply":"2024-08-19T15:46:28.121729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **5.4. Creating train/validate/test sets**\n\nI'm going to split the data into training and testing sets, 80/20.","metadata":{}},{"cell_type":"code","source":"# Splitting the data into training and testing sets\nX_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:46:35.178389Z","iopub.execute_input":"2024-08-19T15:46:35.179620Z","iopub.status.idle":"2024-08-19T15:46:35.191779Z","shell.execute_reply.started":"2024-08-19T15:46:35.179572Z","shell.execute_reply":"2024-08-19T15:46:35.190698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the training set into training and validation sets, 75/25, to result in a final ratio of 60/20/20 for train/validate/test sets.","metadata":{}},{"cell_type":"code","source":"# Splitting the training data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, test_size=0.25, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:46:37.633179Z","iopub.execute_input":"2024-08-19T15:46:37.633642Z","iopub.status.idle":"2024-08-19T15:46:37.644981Z","shell.execute_reply.started":"2024-08-19T15:46:37.633607Z","shell.execute_reply":"2024-08-19T15:46:37.643889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting shape of each training, validation, and testing set to confirm that they are in alignment.\nX_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:46:40.513070Z","iopub.execute_input":"2024-08-19T15:46:40.513531Z","iopub.status.idle":"2024-08-19T15:46:40.521872Z","shell.execute_reply.started":"2024-08-19T15:46:40.513494Z","shell.execute_reply":"2024-08-19T15:46:40.520566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **5.5. Building the Model**\n\n### **5.5.1. Random Forest Model**","metadata":{}},{"cell_type":"code","source":"# Instantiating the random forest classifier\nrf = RandomForestClassifier(random_state=42)\n\n# Creating a dictionary of hyperparameters to tune\ncv_params = {'max_depth': [5, 7, None],\n             'max_features': [0.3, 0.6],\n            #  'max_features': 'auto'\n             'max_samples': [0.7],\n             'min_samples_leaf': [1,2],\n             'min_samples_split': [2,3],\n             'n_estimators': [75,100,200],\n             }\n\n# Defining a dictionary of scoring metrics to capture\nscoring = {'accuracy', 'precision', 'recall', 'f1'}\n\n# Instantiating the GridSearchCV object\nrf_cv = GridSearchCV(rf, cv_params, scoring=scoring, cv=5, refit='recall')","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:46:44.075508Z","iopub.execute_input":"2024-08-19T15:46:44.075900Z","iopub.status.idle":"2024-08-19T15:46:44.083291Z","shell.execute_reply.started":"2024-08-19T15:46:44.075872Z","shell.execute_reply":"2024-08-19T15:46:44.082030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# This may take up to 10 min to run\nrf_cv.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:47:40.667578Z","iopub.execute_input":"2024-08-19T15:47:40.668459Z","iopub.status.idle":"2024-08-19T15:56:24.199271Z","shell.execute_reply.started":"2024-08-19T15:47:40.668419Z","shell.execute_reply":"2024-08-19T15:56:24.198142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Examining best recall score\nrf_cv.best_score_","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:56:38.678273Z","iopub.execute_input":"2024-08-19T15:56:38.679073Z","iopub.status.idle":"2024-08-19T15:56:38.685533Z","shell.execute_reply.started":"2024-08-19T15:56:38.679041Z","shell.execute_reply":"2024-08-19T15:56:38.684454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Examine best parameters\nrf_cv.best_params_","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:57:09.770559Z","iopub.execute_input":"2024-08-19T15:57:09.771583Z","iopub.status.idle":"2024-08-19T15:57:09.778354Z","shell.execute_reply.started":"2024-08-19T15:57:09.771536Z","shell.execute_reply":"2024-08-19T15:57:09.777077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a table of results\n\ndef make_results(model_name, model_object):\n    \n      # Get all the results from the CV and put them in a df\n    cv_results = pd.DataFrame(model_object.cv_results_)\n    \n      # Isolate the row of the df with the max(mean precision score)\n    best_estimator_results = cv_results.iloc[cv_results['mean_test_recall'].idxmax(), :]\n    \n      # Extract accuracy, precision, recall and f1 score from that row\n    f1 = best_estimator_results.mean_test_f1\n    recall = best_estimator_results.mean_test_recall\n    precision = best_estimator_results.mean_test_precision\n    accuracy = best_estimator_results.mean_test_accuracy\n    \n      # Create table of results\n    table = pd.DataFrame({'Model': [model_name],\n                          'Recall': [recall],\n                          'F1': [f1],\n                          'Precision': [precision],\n                          'Accuracy': [accuracy]\n                         }\n                        )\n    return table","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:02:30.360416Z","iopub.execute_input":"2024-08-19T16:02:30.361373Z","iopub.status.idle":"2024-08-19T16:02:30.368317Z","shell.execute_reply.started":"2024-08-19T16:02:30.361338Z","shell.execute_reply":"2024-08-19T16:02:30.367309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_cv_results = make_results('Random Forest CV', rf_cv)\nrf_cv_results","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:02:35.661651Z","iopub.execute_input":"2024-08-19T16:02:35.662760Z","iopub.status.idle":"2024-08-19T16:02:35.677875Z","shell.execute_reply.started":"2024-08-19T16:02:35.662723Z","shell.execute_reply":"2024-08-19T16:02:35.676835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This model performs exceptionally well, with an average recall score of 0.993 across the five cross-validation folds. After checking the precision score to be sure the model is not classifying all samples as claims, it is clear that this model is making almost perfect classifications.\n\n### **5.5.1. XGBoost Model**","metadata":{}},{"cell_type":"code","source":"# Instantiating the XGBoost classifier\nxgb = XGBClassifier(objective='binary:logistic', random_state=0)\n\n# Creating a dictionary of hyperparameters to tune\ncv_params = {'max_depth': [4,8,12],\n             'min_child_weight': [3, 5],\n             'learning_rate': [0.01, 0.1],\n             'n_estimators': [300, 500]\n             }\n\n# Defining a dictionary of scoring metrics to capture\nscoring = {'accuracy', 'precision', 'recall', 'f1'}\n\n# Instantiating the GridSearchCV object\nxgb_cv = GridSearchCV(xgb, cv_params, scoring=scoring, cv=5, refit='recall')","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:59:04.996510Z","iopub.execute_input":"2024-08-19T15:59:04.997513Z","iopub.status.idle":"2024-08-19T15:59:05.004040Z","shell.execute_reply.started":"2024-08-19T15:59:04.997459Z","shell.execute_reply":"2024-08-19T15:59:05.002965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# This may take up to 10 min to run\nxgb_cv.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:59:16.370805Z","iopub.execute_input":"2024-08-19T15:59:16.371225Z","iopub.status.idle":"2024-08-19T16:00:15.657674Z","shell.execute_reply.started":"2024-08-19T15:59:16.371194Z","shell.execute_reply":"2024-08-19T16:00:15.656732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cv.best_score_","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:00:33.147501Z","iopub.execute_input":"2024-08-19T16:00:33.147894Z","iopub.status.idle":"2024-08-19T16:00:33.155150Z","shell.execute_reply.started":"2024-08-19T16:00:33.147868Z","shell.execute_reply":"2024-08-19T16:00:33.153875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cv.best_params_","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:00:37.837687Z","iopub.execute_input":"2024-08-19T16:00:37.838062Z","iopub.status.idle":"2024-08-19T16:00:37.846357Z","shell.execute_reply.started":"2024-08-19T16:00:37.838037Z","shell.execute_reply":"2024-08-19T16:00:37.845134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_cv_results = make_results('XGBoost CV', xgb_cv)\nrf_cv_results","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:03:24.710941Z","iopub.execute_input":"2024-08-19T16:03:24.711334Z","iopub.status.idle":"2024-08-19T16:03:24.727394Z","shell.execute_reply.started":"2024-08-19T16:03:24.711306Z","shell.execute_reply":"2024-08-19T16:03:24.726267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This model also performs exceptionally well. Although its recall score is very slightly lower than the random forest model's at 0.992, its precision score is perfect.","metadata":{}},{"cell_type":"markdown","source":"# **6. Evaluate Models**\n\n## **6.1.Random forest**","metadata":{}},{"cell_type":"code","source":"# Using the random forest \"best estimator\" model to get predictions on the validation set\ny_pred = rf_cv.best_estimator_.predict(X_val)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:04:02.762326Z","iopub.execute_input":"2024-08-19T16:04:02.763301Z","iopub.status.idle":"2024-08-19T16:04:02.796693Z","shell.execute_reply.started":"2024-08-19T16:04:02.763264Z","shell.execute_reply":"2024-08-19T16:04:02.795643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a confusion matrix to visualize the results of the classification model\n\n# Compute values for confusion matrix\nlog_cm = confusion_matrix(y_val, y_pred)\n\n# Create display of confusion matrix\nlog_disp = ConfusionMatrixDisplay(confusion_matrix=log_cm, display_labels=None)\n\n# Plot confusion matrix\nlog_disp.plot()\n\n# Display plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:04:31.467631Z","iopub.execute_input":"2024-08-19T16:04:31.468009Z","iopub.status.idle":"2024-08-19T16:04:31.741635Z","shell.execute_reply.started":"2024-08-19T16:04:31.467982Z","shell.execute_reply":"2024-08-19T16:04:31.740604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The upper-left quadrant displays the number of **true negatives**: the number of opinions that the model accurately classified as so.\n\n* The upper-right quadrant displays the number of **false positives**: the number of opinions that the model misclassified as claims.\n\n* The lower-left quadrant displays the number of **false negatives**: the number of claims that the model misclassified as opinions.\n\n* The lower-right quadrant displays the number of **true positives**: the number of claims that the model accurately classified as so.\n\nA perfect model would yield all true negatives and true positives, and no false negatives or false positives.\n\nAs the above confusion matrix shows, this model does not produce any false positives.\n\nNow, I'm going to create a classification report that includes precision, recall, f1-score, and accuracy metrics to evaluate the performance of the model.","metadata":{}},{"cell_type":"code","source":"# Creating a classification report\n# Creating classification report for random forest model\ntarget_labels = ['opinion', 'claim']\nprint(classification_report(y_val, y_pred, target_names=target_labels))","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:07:38.051511Z","iopub.execute_input":"2024-08-19T16:07:38.052187Z","iopub.status.idle":"2024-08-19T16:07:38.073999Z","shell.execute_reply.started":"2024-08-19T16:07:38.052152Z","shell.execute_reply":"2024-08-19T16:07:38.072673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **6.2. XGBoost**","metadata":{}},{"cell_type":"code","source":"#Evaluate XGBoost model\ny_pred = xgb_cv.best_estimator_.predict(X_val)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:08:15.965119Z","iopub.execute_input":"2024-08-19T16:08:15.965541Z","iopub.status.idle":"2024-08-19T16:08:15.985697Z","shell.execute_reply.started":"2024-08-19T16:08:15.965512Z","shell.execute_reply":"2024-08-19T16:08:15.984650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute values for confusion matrix\nlog_cm = confusion_matrix(y_val, y_pred)\n\n# Create display of confusion matrix\nlog_disp = ConfusionMatrixDisplay(confusion_matrix=log_cm, display_labels=None)\n\n# Plot confusion matrix\nlog_disp.plot()\n\n# Display plot\nplt.title('XGBoost - validation set');\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:08:28.930875Z","iopub.execute_input":"2024-08-19T16:08:28.931296Z","iopub.status.idle":"2024-08-19T16:08:29.232701Z","shell.execute_reply.started":"2024-08-19T16:08:28.931263Z","shell.execute_reply":"2024-08-19T16:08:29.231550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a classification report\ntarget_labels = ['opinion', 'claim']\nprint(classification_report(y_val, y_pred, target_names=target_labels))","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:08:36.719079Z","iopub.execute_input":"2024-08-19T16:08:36.719843Z","iopub.status.idle":"2024-08-19T16:08:36.739798Z","shell.execute_reply.started":"2024-08-19T16:08:36.719807Z","shell.execute_reply":"2024-08-19T16:08:36.738833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The results of the XGBoost model were also nearly perfect. However, its errors tended to be more false negatives. Identifying claims was the priority, so it's important for the model to be good at capturing all actual claim videos. The random forest model has a better recall score and is therefore the champion model.**","metadata":{}},{"cell_type":"markdown","source":"# **7. Using the Champion Model to Predict on Test Data**","metadata":{}},{"cell_type":"code","source":"# Use champion model to predict on test data\ny_pred = rf_cv.best_estimator_.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:10:19.995287Z","iopub.execute_input":"2024-08-19T16:10:19.995749Z","iopub.status.idle":"2024-08-19T16:10:20.030212Z","shell.execute_reply.started":"2024-08-19T16:10:19.995717Z","shell.execute_reply":"2024-08-19T16:10:20.029291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute values for confusion matrix\nlog_cm = confusion_matrix(y_test, y_pred)\n\n# Create display of confusion matrix\nlog_disp = ConfusionMatrixDisplay(confusion_matrix=log_cm, display_labels=None)\n\n# Plot confusion matrix\nlog_disp.plot()\n\n# Display plot\nplt.title('Random forest - test set');\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:10:26.527978Z","iopub.execute_input":"2024-08-19T16:10:26.528707Z","iopub.status.idle":"2024-08-19T16:10:26.764882Z","shell.execute_reply.started":"2024-08-19T16:10:26.528673Z","shell.execute_reply":"2024-08-19T16:10:26.763866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **8. Feature Importance**","metadata":{}},{"cell_type":"code","source":"importances = rf_cv.best_estimator_.feature_importances_\nrf_importances = pd.Series(importances, index=X_test.columns)\n\nfig, ax = plt.subplots()\nrf_importances.plot.bar(ax=ax)\nax.set_title('Feature importances')\nax.set_ylabel('Mean decrease in impurity')\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:10:58.373315Z","iopub.execute_input":"2024-08-19T16:10:58.374415Z","iopub.status.idle":"2024-08-19T16:10:58.841810Z","shell.execute_reply.started":"2024-08-19T16:10:58.374379Z","shell.execute_reply":"2024-08-19T16:10:58.840715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The most predictive features are all related to engagement levels generated by the video. This is not unexpected, as analysis from prior EDA pointed to this conclusion.**","metadata":{}},{"cell_type":"markdown","source":"# **9. Conclusion**\n\nBoth model architectures—random forest (RF) and XGBoost—performed exceptionally well. The RF model had a better recall score (0.993) and was selected as champion.\n\nPerformance on the test holdout data yielded near perfect scores, with only 13 misclassified samples out of 3,817.\n\nSubsequent analysis indicated that, as expected, the primary predictors were all related to video engagement levels, with video view count, like count, share count, and download count accounting for nearly all predictive signal in the data. With these results, we can conclude that videos with higher user engagement levels were much more likely to be claims. In fact, no opinion video had more than 10,000 views.\n\nAs noted, the model performed exceptionally well on the test holdout data. Before deploying the model, I recommend further evaluation using additional subsets of user data. Furthermore, I recommend monitoring the distributions of video engagement levels to ensure that the model remains robust to fluctuations in its most predictive features.","metadata":{}}]}